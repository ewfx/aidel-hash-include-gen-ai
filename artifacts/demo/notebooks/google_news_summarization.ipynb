{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnvs1rdZCDr1",
        "outputId": "3765bf41-2a81-4623-b5b4-765868c0847c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GoogleNews\n",
            "  Downloading GoogleNews-1.6.15-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from GoogleNews) (4.13.3)\n",
            "Collecting dateparser (from GoogleNews)\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from GoogleNews) (2.8.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->GoogleNews) (2.6)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->GoogleNews) (2025.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser->GoogleNews) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->GoogleNews) (5.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->GoogleNews) (1.17.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading GoogleNews-1.6.15-py3-none-any.whl (8.8 kB)\n",
            "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, dateparser, trio-websocket, GoogleNews, selenium\n",
            "Successfully installed GoogleNews-1.6.15 dateparser-1.2.1 outcome-1.3.0.post0 selenium-4.30.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install GoogleNews selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from GoogleNews import GoogleNews\n",
        "import pandas as pd\n",
        "\n",
        "user_request = \"\"\"(\"Alphabet\" OR \"Google\" OR \"Alphabet Inc.\") AND\n",
        "(\"financial anomaly\" OR \"financial irregularity\" OR \"regulatory filing\" OR \"legal action\" OR\n",
        "\"financial fraud\" OR \"accounting fraud\" OR \"sanctions\" OR \"SEC investigation\" OR \"audit failure\" OR\n",
        "\"class action lawsuit\" OR \"insider trading\" OR \"whistleblower complaint\" OR \"penalty\" OR \"fine\" OR\n",
        "\"restatement\" OR \"regulatory scrutiny\" OR \"compliance violation\" OR \"litigation\" OR\n",
        "\"shareholder lawsuit\" OR \"corporate governance violation\" OR \"misconduct\" OR \"stock manipulation\")\"\"\"\n",
        "\n",
        "googlenews = GoogleNews(period='7d')\n",
        "googlenews.search(user_request)\n",
        "\n",
        "all_results = []\n",
        "for i in range(1, 10):\n",
        "    googlenews.getpage(i)\n",
        "    result = googlenews.result()\n",
        "    if result:\n",
        "        for item in result:\n",
        "            all_results.append(item)\n",
        "            if len(all_results) >= 10:\n",
        "                break\n",
        "\n",
        "df = pd.DataFrame(all_results)\n",
        "df = df.drop_duplicates(subset=['title'], keep='last')\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "w_zuz705CN2W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "data = df.drop(columns = ['media', 'date', 'datetime', 'desc', 'img'])\n",
        "latest_links = [re.split(\"&ved\", link)[0] for link in df['link']]\n",
        "print(latest_links)\n",
        "print(data.shape)\n",
        "print(data.columns)\n",
        "for i in latest_links:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpcF2XlNCUpB",
        "outputId": "a3698f89-65f6-459c-f23e-31243e385afd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.socialmediatoday.com/news/meta-google-call-for-trump-assistance-fight-australian-fees/743154/', 'https://www.blackenterprise.com/google-settles-28-million-racial-pay-disparity-class-action-lawsuit/', 'https://www.ruetir.com/2025/03/20/europe-accuses-google-of-violating-the-law-and-could-impose-a-historical-fine/', 'https://www.moneycontrol.com/technology/google-apple-hit-by-eu-regulatory-crackdown-article-12969849.html', 'https://www.channelfutures.com/mergers-acquisitions/google-wiz-acquisition-regulatory-challenges', 'https://www.devdiscourse.com/article/technology/3313789-eu-antitrust-crackdown-google-and-apple-face-regulatory-heat', 'https://www.ndtv.com/world-news/google-accused-of-breaching-european-union-rules-risks-fine-7961831', 'https://www.digitalinformationworld.com/2025/03/google-maps-hit-by-10000-fake-listings.html']\n",
            "(8, 2)\n",
            "Index(['title', 'link'], dtype='object')\n",
            "https://www.socialmediatoday.com/news/meta-google-call-for-trump-assistance-fight-australian-fees/743154/\n",
            "https://www.blackenterprise.com/google-settles-28-million-racial-pay-disparity-class-action-lawsuit/\n",
            "https://www.ruetir.com/2025/03/20/europe-accuses-google-of-violating-the-law-and-could-impose-a-historical-fine/\n",
            "https://www.moneycontrol.com/technology/google-apple-hit-by-eu-regulatory-crackdown-article-12969849.html\n",
            "https://www.channelfutures.com/mergers-acquisitions/google-wiz-acquisition-regulatory-challenges\n",
            "https://www.devdiscourse.com/article/technology/3313789-eu-antitrust-crackdown-google-and-apple-face-regulatory-heat\n",
            "https://www.ndtv.com/world-news/google-accused-of-breaching-european-union-rules-risks-fine-7961831\n",
            "https://www.digitalinformationworld.com/2025/03/google-maps-hit-by-10000-fake-listings.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import tempfile\n",
        "\n",
        "def get_selenium_page_source(url):\n",
        "    \"\"\"Retrieve page content using Selenium in case of 401/403 errors.\"\"\"\n",
        "    temp_profile_dir = tempfile.mkdtemp()\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(f\"--user-data-dir={temp_profile_dir}\")\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(random.uniform(1, 3))  # Random delay to avoid detection\n",
        "        html_content = driver.page_source\n",
        "    except Exception as e:\n",
        "        # print(f\"Error using Selenium for {url}: {e}\")\n",
        "        html_content = None\n",
        "    finally:\n",
        "        driver.quit()\n",
        "    return html_content\n",
        "\n",
        "# Function to fetch and process a URL\n",
        "def fetch_description(url):\n",
        "    try:\n",
        "        # Attempt HTTP GET request\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\",\n",
        "            \"Referer\": \"https://www.google.com/\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Connection\": \"keep-alive\"\n",
        "        }\n",
        "        response = requests.get(url, timeout=10, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            html_content = response.text\n",
        "\n",
        "        elif response.status_code in [401, 403]:\n",
        "            print(f\"Using Selenium for {url} due to status {response.status_code}\")\n",
        "            html_content = get_selenium_page_source(url)\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to retrieve: {url} (Status code: {response.status_code})\")\n",
        "            return \"Failed to retrieve the webpage.\"\n",
        "\n",
        "        # Parse HTML content if available\n",
        "        if html_content:\n",
        "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "            paragraphs = soup.find_all(\"p\")\n",
        "            page_description = \" \".join([p.get_text() for p in paragraphs])\n",
        "            return page_description\n",
        "        else:\n",
        "            return \"Failed to retrieve the webpage.\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error retrieving {url}: {e}\")\n",
        "        return \"Failed to retrieve the webpage.\"\n",
        "\n",
        "\n",
        "# Fetch all descriptions concurrently using ProcessPoolExecutor\n",
        "def fetch_all_descriptions(urls, max_workers):\n",
        "    descriptions = []\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all tasks\n",
        "        future_to_url = {executor.submit(fetch_description, url): url for url in urls}\n",
        "\n",
        "        # Process results as they complete\n",
        "        for future in as_completed(future_to_url):\n",
        "            url = future_to_url[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                descriptions.append(result)\n",
        "            except Exception as e:\n",
        "                # print(f\"Error processing {url}: {e}\")\n",
        "                descriptions.append(\"Failed to retrieve the webpage.\")\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "# Number of parallel processes to use\n",
        "num_workers = max(1, os.cpu_count() - 1)\n",
        "\n",
        "# Start the parallel fetching process\n",
        "start_time = time.time()\n",
        "descriptions = fetch_all_descriptions(latest_links, max_workers=num_workers)\n",
        "end_time = time.time()\n",
        "\n",
        "# Store results in the data dictionary\n",
        "# data = {\"description\": descriptions}\n",
        "\n",
        "print(f\"Fetched {len(descriptions)} descriptions in {end_time - start_time:.2f} seconds.\")\n",
        "data[\"description\"] = descriptions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybk_9OI1CZx5",
        "outputId": "b89a4ab0-23e9-48fc-e270-5f98f169a55a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Selenium for https://www.channelfutures.com/mergers-acquisitions/google-wiz-acquisition-regulatory-challenges due to status 403\n",
            "Using Selenium for https://www.ndtv.com/world-news/google-accused-of-breaching-european-union-rules-risks-fine-7961831 due to status 403\n",
            "Fetched 8 descriptions in 23.44 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load summarization model (BART or other models)\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Summarize text with better handling of large inputs\n",
        "def summarize_text(text, max_length=500, chunk_size=2000):\n",
        "    # Split into manageable chunks\n",
        "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    # Summarize each chunk\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=max_length, min_length=50, do_sample=False)[0]['summary_text']\n",
        "        summaries.append(summary)\n",
        "\n",
        "    # Combine all chunk summaries\n",
        "    combined_summary = \" \".join(summaries)\n",
        "\n",
        "    # Optional: Hierarchical summarization to refine final summary\n",
        "    if len(combined_summary) > 2000:  # Chunk limit\n",
        "        combined_summary = summarizer(combined_summary, max_length=max_length, min_length=50, do_sample=False)[0]['summary_text']\n",
        "\n",
        "    return combined_summary\n",
        "\n",
        "input_text = \" \".join(data['description'])\n",
        "summary = summarize_text(input_text)\n",
        "print(f\"Summary: {summary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g1H88m8gCkcq",
        "outputId": "951287f1-5410-4644-e304-1d01c3a89563"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 500, but your input_length is only 403. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=201)\n",
            "Your max_length is set to 500, but your input_length is only 384. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=192)\n",
            "Your max_length is set to 500, but your input_length is only 419. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=209)\n",
            "Your max_length is set to 500, but your input_length is only 371. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=185)\n",
            "Your max_length is set to 500, but your input_length is only 352. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=176)\n",
            "Your max_length is set to 500, but your input_length is only 432. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=216)\n",
            "Your max_length is set to 500, but your input_length is only 376. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=188)\n",
            "Your max_length is set to 500, but your input_length is only 433. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=216)\n",
            "Your max_length is set to 500, but your input_length is only 416. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=208)\n",
            "Your max_length is set to 500, but your input_length is only 106. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  Meta is among a range of tech companies, including Google, Apple, and X, which co-signed a request for the U.S. Government to help them push back against what they’ve labeled “discriminatory” Australian media laws. Back in 2021, the Australian government implemented its “News Media Bargaining Code” which effectively forces social apps and search engines to pay local publishers.\n"
          ]
        }
      ]
    }
  ]
}